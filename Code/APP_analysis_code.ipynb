{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploring American Presidency Project Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.chdir(r\"C:\\Users\\shime\\OneDrive\\Documents\\GitHub\\Final-Project-GroupJavaWockeez\\Code\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### We scraped the data using the following scraper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://www.presidency.ucsb.edu\"\n",
    "LIST_PATH = \"/documents/app-categories/statements\"\n",
    "LIST_URL = BASE_URL + LIST_PATH\n",
    "OUTFILE = \"presidential_statements.csv\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; Scraper/1.0)\"}\n",
    "TEMP_SAVE_EVERY = 100   # flush every N records\n",
    "DELAY_BETWEEN_REQUESTS = 0.35\n",
    "\n",
    "def fetch(url, timeout=15, tries=3):\n",
    "    for attempt in range(tries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            print(f\"Fetch error ({attempt+1}/{tries}) for {url}: {e}\")\n",
    "            time.sleep(1)\n",
    "    return None\n",
    "\n",
    "def extract_detail_content(detail_url):\n",
    "    r = fetch(detail_url)\n",
    "    if not r:\n",
    "        return \"\", \"\", \"\"\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    content_node = s.select_one(\"div.field-docs-content\")\n",
    "    content = content_node.get_text(\"\\n\", strip=True) if content_node else \"\"\n",
    "\n",
    "    # categories (if present)\n",
    "    cats = s.select(\"div.group-meta a, div.field-ds-filed-under- a, .field-ds-filed-under a\")\n",
    "    categories = \", \".join([c.get_text(strip=True) for c in cats]) if cats else \"\"\n",
    "\n",
    "    # citation\n",
    "    cit = s.select_one(\".field-prez-document-citation, .ucsbapp_citation\")\n",
    "    citation = cit.get_text(\" \", strip=True) if cit else \"\"\n",
    "\n",
    "    return content, categories, citation\n",
    "\n",
    "def read_existing_urls(outfile):\n",
    "    p = Path(outfile)\n",
    "    if not p.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return {row.get(\"url\",\"\").strip() for row in reader if row.get(\"url\")}\n",
    "    except Exception as e:\n",
    "        print(\"Error reading existing CSV, will start fresh:\", e)\n",
    "        return set()\n",
    "\n",
    "def scrape_statements(max_pages=None):\n",
    "    existing_urls = read_existing_urls(OUTFILE)\n",
    "    page = 0\n",
    "    total_saved = 0\n",
    "\n",
    "    # prepare CSV writer (append mode)\n",
    "    headers = [\"title\", \"url\", \"president\", \"date\", \"content\", \"categories\", \"citation\"]\n",
    "    outfile_path = Path(OUTFILE)\n",
    "    write_header = not outfile_path.exists()\n",
    "\n",
    "    csvfile = open(outfile_path, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(csvfile, \n",
    "                            fieldnames=headers,\n",
    "                            quoting = csv.QUOTE_ALL,\n",
    "                            escapechar='\\\\')\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if max_pages is not None and page >= max_pages:\n",
    "                print(\"Reached max_pages limit, stopping.\")\n",
    "                break\n",
    "\n",
    "            page_url = f\"{LIST_URL}?page={page}\"\n",
    "            resp = fetch(page_url)\n",
    "            if not resp:\n",
    "                print(\"Failed to fetch listing page:\", page_url)\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # listing item containers for statements\n",
    "            items = soup.select(\"div.views-row, div.node-teaser, div.node-documents.node-teaser\")\n",
    "            # filter duplicates and ensure items have a link\n",
    "            items = [it for it in items if it.select_one(\"a[href*='/documents/']\")]\n",
    "\n",
    "            if not items:\n",
    "                print(f\"No items found on page {page}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            num_items = len(items)\n",
    "            print(f\"Scraping page {page+1}: {num_items} statements (Total so far: {total_saved + num_items})\")\n",
    "\n",
    "            for item in items:\n",
    "                # Title + link\n",
    "                title_a = item.select_one(\".field-title a, h3 a, a[href*='/documents/']\")\n",
    "                if not title_a:\n",
    "                    print(\"Skipping item (no title link). snippet:\", item.get_text(\" \", strip=True)[:150])\n",
    "                    continue\n",
    "                title = title_a.get_text(strip=True)\n",
    "                href = title_a.get(\"href\", \"\").strip()\n",
    "                full_link = href if href.startswith(\"http\") else BASE_URL + href\n",
    "\n",
    "                # skip if already scraped\n",
    "                if full_link in existing_urls:\n",
    "                    # print(\"Skipping already-saved:\", full_link)\n",
    "                    continue\n",
    "\n",
    "                # president (the \"Related\" link on the right column)\n",
    "                pres_a = item.select_one(\".col-sm-4 a, .views-field-field-president a, .field-title ~ .col-sm-4 a\")\n",
    "                president = pres_a.get_text(strip=True) if pres_a else \"\"\n",
    "\n",
    "                # date\n",
    "                date_span = item.select_one(\"span.date-display-single, .views-field-field-docs-date span, .views-field-created span\")\n",
    "                date = date_span.get(\"content\", date_span.get_text(strip=True)) if date_span else \"\"\n",
    "\n",
    "                # fetch detail content\n",
    "                content, categories, citation = extract_detail_content(full_link)\n",
    "\n",
    "                row = {\n",
    "                    \"title\": title,\n",
    "                    \"url\": full_link,\n",
    "                    \"president\": president,\n",
    "                    \"date\": date,\n",
    "                    \"content\": content,\n",
    "                    \"categories\": categories,\n",
    "                    \"citation\": citation\n",
    "                }\n",
    "\n",
    "                writer.writerow(row)\n",
    "                existing_urls.add(full_link)\n",
    "                total_saved += 1\n",
    "\n",
    "                if total_saved % TEMP_SAVE_EVERY == 0:\n",
    "                    csvfile.flush()\n",
    "                    print(f\"Checkpoint: saved {total_saved} records so far.\")\n",
    "\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "            page += 1\n",
    "            # small delay between pages\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    finally:\n",
    "        csvfile.close()\n",
    "\n",
    "    print(\"Finished. Total new records saved:\", total_saved)\n",
    "    return total_saved\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For a quick test set max_pages=2\n",
    "    # For full run use max_pages=None\n",
    "    scrape_statements(max_pages=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original scraper had issues with formatting columns, so we reformat here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_final = pd.read_csv(\"presidential_statements.csv\", header=None)\n",
    "\n",
    "df_final.columns = [\n",
    "    \"title\",\n",
    "    \"url\",\n",
    "    \"president\",\n",
    "    \"date\",\n",
    "    \"content\",\n",
    "    \"categories\",\n",
    "    \"citation\"\n",
    "]\n",
    "\n",
    "df_final.to_csv(\"presidential_statements_scraped.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# From here below I'm working on the American Presidency presidential statements csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12399, 7)\n",
      "                                               title  \\\n",
      "0            Joint Statement on U.S.â€“Ukraine Meeting   \n",
      "1  Statement on Signing the Epstein Files Transpa...   \n",
      "2  Joint Statement on a Framework for a United St...   \n",
      "3  Joint Statement on a Framework for United Stat...   \n",
      "4  Joint Statement on a Framework for United Stat...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.presidency.ucsb.edu/documents/join...   \n",
      "1  https://www.presidency.ucsb.edu/documents/stat...   \n",
      "2  https://www.presidency.ucsb.edu/documents/join...   \n",
      "3  https://www.presidency.ucsb.edu/documents/join...   \n",
      "4  https://www.presidency.ucsb.edu/documents/join...   \n",
      "\n",
      "                    president                       date  \\\n",
      "0  Donald J. Trump (2nd Term)  2025-11-23T00:00:00+00:00   \n",
      "1  Donald J. Trump (2nd Term)  2025-11-19T00:00:00+00:00   \n",
      "2  Donald J. Trump (2nd Term)  2025-11-14T00:00:00+00:00   \n",
      "3  Donald J. Trump (2nd Term)  2025-11-13T00:00:00+00:00   \n",
      "4  Donald J. Trump (2nd Term)  2025-11-13T00:00:00+00:00   \n",
      "\n",
      "                                             content  \\\n",
      "0  On 23 November 2025, representatives of the Un...   \n",
      "1  Jeffrey Epstein, who was charged by the Trump ...   \n",
      "2  Today, the United States of America (United St...   \n",
      "3  The United States of America (United States, o...   \n",
      "4  President Donald J. Trump and President Daniel...   \n",
      "\n",
      "                                          categories  \\\n",
      "0                    Presidential, Statements, Joint   \n",
      "1  Presidential, Statements, Signing Statements, ...   \n",
      "2                    Presidential, Statements, Joint   \n",
      "3                    Presidential, Statements, Joint   \n",
      "4                    Presidential, Statements, Joint   \n",
      "\n",
      "                                            citation  \n",
      "0  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "1  Donald J. Trump (2nd Term), Statement on Signi...  \n",
      "2  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "3  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "4  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "Index(['title', 'url', 'president', 'date', 'content', 'categories',\n",
      "       'citation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Read in the CSV file\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"presidential_statements_scraped.csv\")\n",
    "\n",
    "#Inspect the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "#Print the first few rows\n",
    "print(dataset.head())\n",
    "\n",
    "#Check columns\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Adding features for future visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'url', 'president', 'date', 'content', 'categories',\n",
      "       'citation'],\n",
      "      dtype='object')\n",
      "['Donald J. Trump (2nd Term)' 'Gavin Newsom' 'Joseph R. Biden, Jr.'\n",
      " 'Donald J. Trump (1st Term)' 'Barack Obama' 'George W. Bush'\n",
      " 'William J. Clinton' 'George Bush' 'Ronald Reagan' 'Jimmy Carter'\n",
      " 'Gerald R. Ford' 'Richard Nixon' 'Lyndon B. Johnson' 'John F. Kennedy'\n",
      " 'Dwight D. Eisenhower' 'Harry S Truman' 'Franklin D. Roosevelt'\n",
      " 'Herbert Hoover' 'Calvin Coolidge' 'Warren G. Harding' 'Woodrow Wilson'\n",
      " 'William Howard Taft' 'Theodore Roosevelt' 'William McKinley'\n",
      " 'Grover Cleveland']\n",
      "25\n",
      "46\n",
      "                        president       party\n",
      "1890                 Barack Obama    Democrat\n",
      "12383             Calvin Coolidge  Republican\n",
      "1497   Donald J. Trump (1st Term)  Republican\n",
      "0      Donald J. Trump (2nd Term)  Republican\n",
      "10773        Dwight D. Eisenhower  Republican\n",
      "11805       Franklin D. Roosevelt    Democrat\n",
      "40                   Gavin Newsom         NaN\n",
      "6601                  George Bush  Republican\n",
      "3321               George W. Bush  Republican\n",
      "8533               Gerald R. Ford  Republican\n",
      "12398            Grover Cleveland    Democrat\n",
      "11336              Harry S Truman    Democrat\n",
      "12131              Herbert Hoover  Republican\n",
      "7923                 Jimmy Carter    Democrat\n",
      "10446             John F. Kennedy    Democrat\n",
      "72           Joseph R. Biden, Jr.    Democrat\n",
      "9557            Lyndon B. Johnson    Democrat\n",
      "8887                Richard Nixon  Republican\n",
      "7066                Ronald Reagan  Republican\n",
      "12396          Theodore Roosevelt  Republican\n",
      "12389           Warren G. Harding  Republican\n",
      "12395         William Howard Taft  Republican\n",
      "4491           William J. Clinton    Democrat\n",
      "12397            William McKinley  Republican\n",
      "12390              Woodrow Wilson    Democrat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add party affiliation based on president name\n",
    "#Make copy of df to keep original intact\n",
    "dataset_raw = dataset.copy()\n",
    "\n",
    "'''From this point on we will manipulate dataset and keep dataset_raw as original'''\n",
    "\n",
    "\n",
    "#Check dataset again\n",
    "print(dataset.columns)\n",
    "\n",
    "#Check presidents in dataset\n",
    "print(dataset['president'].unique())\n",
    "print(len(dataset['president'].unique()))\n",
    "\n",
    "#Add party affiliation\n",
    "party_affiliation = { \"Donald J. Trump (1st Term)\" : \"Republican\",\n",
    "                        \"Donald J. Trump (2nd Term)\" : \"Republican\",\n",
    "                        \"Joseph R. Biden, Jr.\": \"Democrat\",\n",
    "                        \"Barack Obama\": \"Democrat\",\n",
    "                        \"George W. Bush\": \"Republican\",\n",
    "                        \"William J. Clinton\": \"Democrat\",\n",
    "                        \"George Bush\": \"Republican\",\n",
    "                        \"Ronald Reagan\": \"Republican\",\n",
    "                        \"Jimmy Carter\": \"Democrat\",\n",
    "                        \"Gerald R. Ford\": \"Republican\",\n",
    "                        \"Richard Nixon\": \"Republican\",\n",
    "                        \"Lyndon B. Johnson\": \"Democrat\",\n",
    "                        \"John F. Kennedy\": \"Democrat\",\n",
    "                        \"Dwight D. Eisenhower\": \"Republican\",\n",
    "                        \"Harry S Truman\": \"Democrat\",\n",
    "                        \"Franklin D. Roosevelt\": \"Democrat\",\n",
    "                        \"Herbert Hoover\": \"Republican\", \n",
    "                        \"Calvin Coolidge\": \"Republican\",\n",
    "                        \"Warren G. Harding\": \"Republican\",\n",
    "                        \"Woodrow Wilson\": \"Democrat\",\n",
    "                        \"William Howard Taft\": \"Republican\",\n",
    "                        \"Theodore Roosevelt\": \"Republican\",\n",
    "                        \"William McKinley\": \"Republican\",\n",
    "                        \"Grover Cleveland\": \"Democrat\",\n",
    "                        \"Benjamin Harrison\": \"Republican\",\n",
    "                        \"Chester A. Arthur\": \"Republican\",\n",
    "                        \"James A. Garfield\": \"Republican\",\n",
    "                        \"Rutherford B. Hayes\": \"Republican\",\n",
    "                        \"Ulysses S. Grant\": \"Republican\",\n",
    "                        \"Andrew Johnson\": \"Democrat\",\n",
    "                        \"Abraham Lincoln\": \"Republican\",\n",
    "                        \"James Buchanan\": \"Democrat\",\n",
    "                        \"Franklin Pierce\": \"Democrat\",\n",
    "                        \"Millard Fillmore\": \"Whig\",\n",
    "                        \"Zachary Taylor\": \"Whig\",\n",
    "                        \"James K. Polk\": \"Democrat\",\n",
    "                        \"John Tyler\": \"Whig\",\n",
    "                        \"William Harrison\": \"Whig\",\n",
    "                        \"Martin Van Buren\": \"Democrat\",\n",
    "                        \"Andrew Jackson\": \"Democrat\",\n",
    "                        \"John Quincy Adams\": \"National Republican\",\n",
    "                        \"James Monroe\": \"Democrat-Republican\",\n",
    "                        \"James Madison\": \"Democrat-Republican\",\n",
    "                        \"Thomas Jefferson\": \"Democrat-Republican\",\n",
    "                        \"John Adams\": \"Federalist\",\n",
    "                        \"George Washington\": \"Federalist\"\n",
    "                        }\n",
    "\n",
    "#Check if we have all presidents listed\n",
    "print(len(party_affiliation))\n",
    "\n",
    "#Map party affiliation to df\n",
    "dataset['party'] = dataset['president'].map(party_affiliation)\n",
    "\n",
    "#Check if it correctly mapped\n",
    "print(dataset[['president', 'party']].drop_duplicates().sort_values(by='president'))\n",
    "\n",
    "'''Prior to the two party system we know, there were other parties such as Whig, Federalist, National Republican, and Democrat-Republican. We will keep these as is for now.'''\n",
    "\n",
    "#Remove gavin newsom speeches if any since he is not a president\n",
    "dataset = dataset[dataset['president'] != 'Gavin Newsom']\n",
    "\n",
    "#Check dataset again to see if drop worked\n",
    "\"Gavin Newsom\" in dataset['president'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Dataset is inspected, now we can preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess text\n",
    "\n",
    "#Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # Download if needed\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') #Download if needed\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') #Download if needed\n",
    "\n",
    "#Define stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#Create function to preprocess text\n",
    "def preprocess(text): \n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)  # Remove HTML entities\n",
    "    text = re.sub(r\"[^a-z\\s']\", ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ',text).strip()  # Remove extra whitespace\n",
    "    text = re.sub (r'\\d+', '', text)  # Remove numbers\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    tokens = [word for word in tokens if word not in stopwords]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return ' '.join(tokens)  # Join tokens back to string\n",
    "\n",
    "#Apply to text \n",
    "dataset['cleaned_content'] = dataset['content'].apply(preprocess)\n",
    "\n",
    "#Check df \n",
    "print(dataset[['content', 'cleaned_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling with LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Make new stopword list due to overlap in previous runs\n",
    "stopwords = ['american', 'america', 'states', 'state', 'president']\n",
    "\n",
    "#Vectorize statements \n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stopwords, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(dataset['cleaned_content'])\n",
    "\n",
    "#Extract the topics \n",
    "lda = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "#Function for displaying the topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\"|\".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "#Display topics\n",
    "no_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, no_top_words)\n",
    "\n",
    "#Topic coherence evaluation\n",
    "print(lda.perplexity(X))\n",
    "print(lda.score(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting topic modeling with Gensim LDA\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "#Prepare data for Gensim\n",
    "texts = [doc.split() for doc in dataset['cleaned_content']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#Build LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "#Display topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "#Evaluate topic coherence\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_lda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic modeling again this time with NMF\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Vectorize again using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(dataset['cleaned_content'])\n",
    "\n",
    "#Fit NMF\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(X_tfidf)\n",
    "\n",
    "#Display topics\n",
    "no_top_words = 10 \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(nmf, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Building labels for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Naturally, the dataset doesn't have sentiment labels for us to work with. For this reason it's best that we try and do zero-shot classification and validate the results with another model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment analysis using VADER\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# sia = SentimentIntensityAnalyzer()\n",
    "# dataset['sentiment'] = dataset['cleaned_content'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "# print(dataset[['cleaned_content', 'sentiment']].head())\n",
    "# print(dataset.columns)\n",
    "\n",
    "# # Visualize sentiment distribution\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.histplot(dataset['sentiment'], bins=30, kde=True)\n",
    "# plt.title('Sentiment Distribution of Presidential Statements')\n",
    "# plt.xlabel('Sentiment Score')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1821 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1821) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m labels = [\u001b[33m\"\u001b[39m\u001b[33mpositive\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnegative\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#Apply to dataset \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m dataset[\u001b[33m'\u001b[39m\u001b[33mtransformer_sentiment\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m dataset[\u001b[33m'\u001b[39m\u001b[33mtransformer_sentiment_score\u001b[39m\u001b[33m'\u001b[39m] = dataset[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: sentiment_classifier(x)[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Check results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      6\u001b[39m labels = [\u001b[33m\"\u001b[39m\u001b[33mpositive\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnegative\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#Apply to dataset \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m dataset[\u001b[33m'\u001b[39m\u001b[33mtransformer_sentiment\u001b[39m\u001b[33m'\u001b[39m] = dataset[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43msentiment_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m dataset[\u001b[33m'\u001b[39m\u001b[33mtransformer_sentiment_score\u001b[39m\u001b[33m'\u001b[39m] = dataset[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: sentiment_classifier(x)[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Check results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:168\u001b[39m, in \u001b[36mTextClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    135\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m inputs = (inputs,)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[32m    170\u001b[39m _legacy = \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:199\u001b[39m, in \u001b[36mTextClassificationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters:\n\u001b[32m    198\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:905\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    899\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    903\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    915\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:711\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    709\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    714\u001b[39m     attention_mask = attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Analyzing_political_rhetoric/.venv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:125\u001b[39m, in \u001b[36mEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, input_embeds)\u001b[39m\n\u001b[32m    121\u001b[39m     position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[32m    123\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m embeddings = \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    126\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[32m    127\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.dropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1821) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis with transformers \n",
    "from transformers import pipeline\n",
    "import torch \n",
    "\n",
    "# Initialize pipeline (do NOT set batch_size here)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    framework=\"pt\",\n",
    "    device=device\n",
    ")\n",
    "labels = [\"positive\", \"negative\"]\n",
    "\n",
    "# Batched inference for speed\n",
    "texts = dataset['content'].fillna(\"\").tolist()\n",
    "results = sentiment_classifier(texts, batch_size=32, truncation=True)\n",
    "\n",
    "# Assign results to DataFrame\n",
    "dataset['transformer_sentiment'] = [r['label'] for r in results]\n",
    "dataset['transformer_sentiment_score'] = [r['score'] for r in results]\n",
    "\n",
    "#Check results\n",
    "print(dataset[['content', 'transformer_sentiment', 'transformer_sentiment_score']].head())\n",
    "\n",
    "#Sanity check columns\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### For a more technical analysis, we want to attempt to capture rhetoric more deeply. For this we will use another transformer this time, creating our own custom labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the transformers library\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "labels = [\"polarizing\", \"unifying\"]\n",
    "\n",
    "#Example usage\n",
    "example_text = \"The economy is doing great and we are seeing unprecedented growth.\"\n",
    "result = classifier(example_text, candidate_labels=labels)\n",
    "print(result)\n",
    "\n",
    "#Application to dataset\n",
    "dataset['generated_sentiment'] = dataset['content'].apply(lambda x: classifier(x, candidate_labels=labels)['labels'][0])\n",
    "\n",
    "'''Political polarization analysis based on generated sentiment from zero-shot classification'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement BOW and Logistic Regression Baseline for rhetoric analysis\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Dataset view\n",
    "print(dataset.columns)\n",
    "print(dataset.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
